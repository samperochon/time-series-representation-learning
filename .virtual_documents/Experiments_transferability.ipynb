get_ipython().getoutput("nvidia-smi")


# Load Git folder
import os
from getpass import getpass
import urllib
repo_user = 'nicolas-dufour'
user = input('Github Username: ')
password = getpass('Password: ')
repo_name = 'time-series-representation-learning'
# your password is converted into url format
password = urllib.parse.quote(password)
cmd_string = 'git clone https://{0}:{1}@github.com/{2}/{3}.git'.format(user, password, repo_user, repo_name)
os.system(cmd_string)
cmd_string, password = "", "" # removing the password from the variable
# Bad password fails silently so make sure the repo was copied
assert os.path.exists(f"/content/{repo_name}"), "Incorrect Password or Repo Not Found, please try again"


get_ipython().run_cell_magic("capture", "", """!pip install git+https://github.com/PyTorchLightning/pytorch-lightning
!pip install wandb
!pip install pyts""")


get_ipython().run_cell_magic("capture", "", """!wget https://www.cs.ucr.edu/~eamonn/time_series_data_2018/UCRArchive_2018.zip
!unzip -P 'someone' UCRArchive_2018.zip -d /content/time-series-representation-learning/
!mv /content/time-series-representation-learning/UCRArchive_2018 /content/time-series-representation-learning/data
!rm UCRArchive_2018.zip""")


# Connect to Drive to store results after the night 

from google.colab import drive
drive.mount('/content/drive')
#cd /content/drive/MyDrive/MVA/Gaze/GazeCapture/pytorch
ROOT_DATA = '/content/drive/MyDrive/'











get_ipython().run_cell_magic("capture", "", """!wget http://www.timeseriesclassification.com/Downloads/Archives/Multivariate2018_arff.zip
!unzip -P 'someone' UCRArchive_2018.zip -d /content/time-series-representation-learning/
!mv /content/time-series-representation-learning/Multivariate2018_arff /content/time-series-representation-learning/data
!rm Multivariate2018_arff.zip""")





get_ipython().run_line_magic("cd", " /content/time-series-representation-learning/")


get_ipython().run_line_magic("load_ext", " autoreload")
get_ipython().run_line_magic("autoreload", " 2")

import argparse
import json
import os
import sys
from tqdm.notebook import tqdm
import wandb

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pytorch_lightning as pl
import sklearn
import torch

from datamodule import TimeSeriesDataModule
from loss import TripletLoss
from model import (
    CausalCNN,
    CausalCNNEncoder,
    CausalConvolutionBlock,
    Chomp1d,
    SqueezeChannels,
)
from train import TimeSeriesEmbedder
from utils import load_UCR_dataset

root_data = "./data/"

# The data folder contains the data such that there are:
# ./data/DodgerLoopDay/DodgerLoopDay_TEST.tsv
# ./data/DodgerLoopDay/DodgerLoopDay_TRAIN.tsv


# Optimization parameters
batch_size = 20
num_workers = 2
betas = (0.9, 0.999)
weight_decay = 1e-2
lr = 0.001

# Model parameter
in_channels = 1
channels = 40
depth = 4
reduced_size = 160
out_channels = 320
kernel_size = 3
N_sample = 288

# Data parameters
train_path = os.path.join(root_data, "FordA", "FordA_TRAIN.tsv")
val_path = os.path.join(root_data, "FordA", "FordA_TEST.tsv")

train_path = os.path.join(root_data,'DodgerLoopDay','DodgerLoopDay_TRAIN.tsv')
val_path = os.path.join(root_data,'DodgerLoopDay','DodgerLoopDay_TEST.tsv')


# Datamodule importation
datamodule = TimeSeriesDataModule(
    train_path,
    val_path,
    batch_size,
    num_workers,
    min_length=20,
    multivariate=False,
    fill_na=True,
)
datamodule.setup()
# Model definition
model = TimeSeriesEmbedder(
    in_channels=in_channels,
    channels=channels,
    depth=depth,
    reduced_size=reduced_size,
    out_channels=out_channels,
    kernel_size=kernel_size,
    lr=lr,
    weight_decay=weight_decay,
    betas=betas,
    train_path=train_path,
    test_path=val_path
)


max_steps = 20
checkpoint_callback = pl.callbacks.ModelCheckpoint(
    mode="min",
    monitor="train_loss_epoch",
    dirpath="checkpoints",
    filename="causalcnn-{epoch:02d}-{train_loss_epoch:.2f}",
)

wandb_logger = pl.loggers.WandbLogger(
    project="Self Supervised Time Series LEarning", name="Run n 1"
)
trainer = pl.Trainer(
    #gpus=1,
    max_steps=max_steps,
    num_sanity_val_steps = -1,
    logger=wandb_logger,
    # val_check_interval=50,
    callbacks=[checkpoint_callback],
)


trainer.fit(model, datamodule)


trainer.validate(model)





trainer.test()


import plotly.express as px
px.scatter(model.val_tsne_rep, x="x", y="y", color="labels", animation_frame="step")


trainer.test()


model.compute_scores()


from datamodule import UnivariateTestDataset
from sklearn.manifold import TSNE
import plotly.express as px


train_set = UnivariateTestDataset(val_path, fill_na=True)
embs = model.encoder(torch.Tensor(train_set.time_series[:,None,:])).detach().numpy()
labels = train_set.labels

tsne = TSNE(n_components=2, random_state=21)
projected_emb = pd.DataFrame(np.concatenate((labels[:,None], tsne.fit_transform(embs)), axis=1), columns = ['labels', 'x', 'y'])
projected_emb['labels'] = projected_emb['labels'].astype('int').astype('str')
fig = px.scatter(projected_emb, x = 'x', y='y', color='labels')
fig.write_html("./test_tsne_FordA_without_Train.html", auto_play=True)





from glob import glob 

# Optimization parameters
batch_size = 10
num_workers = 2
betas = (0.9, 0.999)
weight_decay = 1e-2
lr = 0.001

# Model parameter
in_channels = 1
channels = 40
depth = 10
reduced_size = 160
out_channels = 320
kernel_size = 3
N_sample = 288

dataset_name = []
train_score_list = []
test_score_list = []

for dir_path in tqdm(glob(os.path.join(root_data,'*'))):
    
    print('Doing {}'.format(os.path.basename(dir_path)))
    train_path = glob(os.path.join(dir_path,'*TRAIN.tsv'))[0] if len(glob(os.path.join(dir_path,'*TRAIN.tsv')))>0 else None
    test_path = glob(os.path.join(dir_path,'*TEST.tsv'))[0] if len(glob(os.path.join(dir_path,'*TEST.tsv')))>0 else None

    if train_path is None or test_path is None or os.path.basename(dir_path)=='PigAirwayPressure': 
        continue
    

    # Datamodule importation
    datamodule = TimeSeriesDataModule(train_path, test_path, batch_size, num_workers, min_length=20, multivariate=False, fill_na=True)
    # Model definition
    model = TimeSeriesEmbedder(
        in_channels = in_channels,
        channels = channels,
        depth = depth,
        reduced_size = reduced_size,
        out_channels = out_channels,
        kernel_size = kernel_size,
        lr=lr,
        weight_decay = weight_decay,
        betas = betas,
        train_path=train_path,
        test_path=test_path
    )
    
    max_steps = 2000
    checkpoint_callback = pl.callbacks.ModelCheckpoint(
        mode="min",
        monitor="train_loss_epoch",
        dirpath="checkpoints",
        filename="causalcnn-{epoch:02d}-{train_loss_epoch:.2f}",
    )

    
    run = wandb.init(
         project='self-supervised-time-series',
         name='ALL EXPERIMENTS - {}'.format(os.path.basename(dir_path)),
         tags=[os.path.basename(dir_path)],
         group='Sigmoid loss',
         entity='nicolas-dufour-team',
         reinit=True
         )
    wandb_logger = pl.loggers.WandbLogger()
    trainer = pl.Trainer(
        gpus=1,
        max_steps=max_steps,
        num_sanity_val_steps = -1,
        logger=wandb_logger,
        # val_check_interval=50,
        callbacks=[checkpoint_callback],
    )
    model.datamodule = datamodule
    trainer.fit(model, datamodule)
    fig = trainer.test(verbose=False)
    
    # try:
    train_score, test_score = model.compute_scores()
    print('\n', train_score, test_score)
    dataset_name.append(os.path.basename(dir_path))
    wandb.log({'Train score': train_score})
    wandb.log({'Test score': test_score})
    train_score_list.append(train_score)
    test_score_list.append(test_score)
    # except:
    #     print('Dataset: {} does not have enough example per class (will fix that)'.format(os.path.basename(dir_path)))
    run.finish()
results = pd.DataFrame(np.array([dataset_name,train_score_list, test_score_list]).transpose(), columns=['Name', 'Train Accuracy', 'Test Accuracy'])
results.to_csv('./results_univariate_without_training.csv')


from pyts.datasets import load_basic_motions, uea_dataset_list, fetch_uea_dataset

# Optimization parameters 
batch_size = 10
num_workers = 4
betas = (0.9, 0.999)
weight_decay = 1e-2
lr = 0.001

# Model parameter
in_channels = 1
channels = 40
depth = 10
reduced_size = 160
out_channels = 320
kernel_size = 3
loss = "xent"
temp = 1



dataset_names = ['BasicMotions', 'Cricket','EigenWorms', 'LSST','RacketSports', 'SelfRegulationSCP1']

dataset_name = 'SelfRegulationSCP1'

# Datamodule importation
datamodule = TimeSeriesDataModule('', '', batch_size, num_workers, dataset_name, min_length=20, multivariate=True, fill_na=True)
datamodule.setup()

in_channels = datamodule.train_set.time_series.shape[1]

# Model definition
model = TimeSeriesEmbedder(
    in_channels = in_channels,
    channels = channels,
    depth = depth,
    reduced_size = reduced_size,
    out_channels = out_channels,
    kernel_size = kernel_size,
    lr=lr,
    weight_decay = weight_decay,
    betas = betas,
    train_path='',
    test_path='', 
    dataset_name=dataset_name,
    multivariate=True, 
    loss = loss,
    temp = temp)

model.datamodule = datamodule



max_steps = 2000
checkpoint_callback = pl.callbacks.ModelCheckpoint(
    mode="min",
    monitor="train_loss_epoch",
    dirpath="checkpoints",
    filename="causalcnn-{epoch:02d}-{train_loss_epoch:.2f}",
)

run = wandb.init(
            project='self-supervised-time-series',
            name='ALL EXPERIMENTS - {}'.format(dataset_name),
            tags=[dataset_name, 'Multivariate'],
            group='Multivariate Transferability',
            entity='nicolas-dufour-team',
            reinit=True
            )
wandb_logger = pl.loggers.WandbLogger()

trainer = pl.Trainer(
    gpus=1,
    max_steps=max_steps,
    num_sanity_val_steps = -1,
    logger=wandb_logger,
    #val_check_interval=50,
    callbacks=[checkpoint_callback],
)


trainer.fit(model, datamodule)


trainer.validate(model)


# try:
train_score, test_score = compute_accuracy(model, datamodule)
print("\n", train_score, test_score)
dataset_name.append(os.path.basename(dir_path))
wandb.log({"Train score": train_score})
wandb.log({"Test score": test_score})
train_score_list.append(train_score)
test_score_list.append(test_score)
run.finish()








train_score, test_score = model.compute_scores()
print('\n', train_score, test_score)
dataset_name.append(dataset_name)
wandb.log({'Train score': train_score})
wandb.log({'Test score': test_score})
train_score_list.append(train_score)
test_score_list.append(test_score)
# except:
#     print('Dataset: {} does not have enough example per class (will fix that)'.format(os.path.basename(dir_path)))
run.finish()

results = pd.DataFrame(np.array([dataset_name, train_score_list, test_score_list]).transpose(),columns=["Name", "Train Accuracy", "Test Accuracy"],)
results.to_csv("./results_transferability_multivariate.csv")


import plotly.express as px
px.scatter(model.val_tsne_rep, x="x", y="y", color="labels", animation_frame="step")



model.encoder.cuda()
model.compute_scores()


from datamodule import UnivariateTestDataset
from sklearn.manifold import TSNE
import plotly.express as px


embs = model.encoder(torch.Tensor(datamodule.test_set.time_series).cuda()).cpu().detach().numpy()
labels = datamodule.test_set.labels

tsne = TSNE(n_components=2, random_state=21)
projected_emb = pd.DataFrame(np.concatenate((labels[:,None], tsne.fit_transform(embs)), axis=1), columns = ['labels', 'x', 'y'])
projected_emb['labels'] = projected_emb['labels'].astype('int').astype('str')
fig = px.scatter(projected_emb, x = 'x', y='y', color='labels')
#fig.write_html("./test_tsne_FordA_without_Train.html", auto_play=True)
fig.show()


from sklearn.model_selection import (GridSearchCV,train_test_split, cross_val_score)
from sklearn.svm import SVC
def compute_scores_transfer(model, datamodule):
    '''
        Compute SVM accuracy score on the train then test set. Is sufficient train data, SVM C hyperparameters is found using grid search.  
    '''

    train_loader = datamodule.val_dataloader()
    test_loader = datamodule.test_dataloader()
    train_emb = list()
    train_labels = list()
    if not model.multivariate:
        for _, (labels, train_series) in enumerate(train_loader):
            train_emb.append(model(train_series[:,None,:].cuda()).cpu().detach().numpy())
            train_labels.append(labels.numpy())
        train_emb = np.concatenate(train_emb)
        train_labels = np.concatenate(train_labels)
    else:
        for _, (labels, train_series) in enumerate(train_loader):
            train_emb.append(model(train_series.cuda()).cpu().detach().numpy())
            train_labels.append(labels.numpy())
        train_emb = np.concatenate(train_emb)
        train_labels = np.concatenate(train_labels)

    
    nb_classes = len(np.unique(train_labels))
    train_size = train_emb.shape[0]

    classifier = SVC(C=np.inf, gamma='scale')
    if train_size // nb_classes < 5 or train_size < 50:
        classifier = classifier.fit(train_emb, train_labels)
    else:
        grid_search = GridSearchCV(classifier, {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000,np.inf],
                                                                        'kernel': ['linear'],'degree': [3],'gamma': ['scale'],'coef0': [0],'shrinking': [True],'probability': [False],'tol': [0.001],
                                                                            'cache_size': [200], 'class_weight': [None],'verbose': [False],'max_iter': [10000000],
                                                                        'decision_function_shape': ['ovr'],'random_state': [None]}, cv=5, n_jobs=5)
        if train_size <= 10000:
            grid_search.fit(train_emb, train_labels)
        else:
            # If the training set is too large, subsample 10000 train
            # examples
            split = train_test_split(
                train_emb, train_labels,
                train_size=10000, random_state=0, stratify=y
            )
            grid_search.fit(split[0], split[2])
        classifier = grid_search.best_estimator_

    # Cross validation score
    train_score = np.mean(cross_val_score(classifier, train_emb, y=train_labels, cv=5, n_jobs=5))
    
    
    # Predict class for the test set
    if not model.multivariate:
        test_emb = list()
        test_labels = list()
        for _, (labels, test_series) in enumerate(test_loader):
            test_emb.append(model(test_series[:,None,:].cuda()).cpu().detach().numpy())
            test_labels.append(labels.numpy())
        test_emb = np.concatenate(test_emb)
        test_labels = np.concatenate(test_labels)
    else:
        test_emb = list()
        test_labels = list()
        for _, (labels, test_series) in enumerate(test_loader):
            test_emb.append(model(test_series.cuda()).cpu().detach().numpy())
            test_labels.append(labels.numpy())
        test_emb = np.concatenate(test_emb)
        test_labels = np.concatenate(test_labels)

    
    test_score = np.mean(cross_val_score(classifier, test_emb, y=test_labels, cv=5, n_jobs=5))

    
    return train_score, test_score


dataset_names_to_test = ['BasicMotions', 'Cricket','EigenWorms', 'LSST','RacketSports', 'SelfRegulationSCP1']


train_score_list = []
test_score_list = []
dataset_names = []

for dataset_name in dataset_names_to_test:
    print('Doing {}'.format(dataset_name))
    #if dataset_name in ['DuckDuckGeese', 'EigenWorms', 'Ering']: # First one gives *** second one gives cuda outof memory (managable), third is 'non a zip file
    #    dataset_names.append(dataset_name)
    #    train_score_list.append(np.nan)
    #    test_score_list.append(np.nan)
    #    continue


    # Datamodule importation
    datamodule = TimeSeriesDataModule('', '', batch_size, num_workers, dataset_name, min_length=20, multivariate=True, fill_na=True)
    datamodule.setup()

    in_channels = datamodule.train_set.time_series.shape[1]

    run = wandb.init(
            project="self-supervised-time-series",
            name="Transfert - {}".format(os.path.basename(dir_path)),
            tags=[os.path.basename(dir_path)],
            group="Transfert from FordA",
            entity="nicolas-dufour-team",
            reinit=True,
        )
    # try:
    train_score, test_score = compute_accuracy(model, datamodule)
    print("\n", train_score, test_score)
    dataset_name.append(os.path.basename(dir_path))
    wandb.log({"Train score": train_score})
    wandb.log({"Test score": test_score})
    train_score_list.append(train_score)
    test_score_list.append(test_score)
    # except:
    #     print('Dataset: {} does not have enough example per class (will fix that)'.format(os.path.basename(dir_path)))
    run.finish()
    results = pd.DataFrame(
        np.array([dataset_name, train_score_list, test_score_list]).transpose(),
        columns=["Name", "Train Accuracy", "Test Accuracy"],
    )
    results.to_csv("./results_transferability_multivariate.csv")

    #except:

        # dataset_names.append(dataset_name)
        # train_score_list.append(np.nan)
        # test_score_list.append(np.nan)
        # results = pd.DataFrame(np.array([dataset_names,train_score_list, test_score_list]).transpose(), columns=['Name', 'Train Accuracy', 'Test Accuracy'])
        # results.to_csv('./results_transfer.csv')
        # results.to_csv('/content/drive/MyDrive/results_transfer.csv')
        # continue



datamodule.setup(0)
train_dataloader = datamodule.train_dataloader()
test_dataloader = datamodule.test_dataloader()

labels, test_time_series = next(iter(test_dataloader))
ref_time_series, pos_time_series = next(iter(train_dataloader))


# Debug to look at the data

datamodule.setup(0)
train_dataloader = datamodule.train_dataloader()
test_dataloader = datamodule.test_dataloader()

labels, test_time_series = next(iter(test_dataloader))
ref_time_series, pos_time_series = next(iter(train_dataloader))

X_train, X_test , y_train , y_test = fetch_uea_dataset(dataset_name, use_cache=True, return_X_y=True)


from pyts.datasets import uea_dataset_list, fetch_uea_dataset

from glob import glob 

# Optimization parameters
batch_size = 10
num_workers = 2
betas = (0.9, 0.999)
weight_decay = 1e-2
lr = 0.001

# Model parameter
in_channels = 1
channels = 40
depth = 10
reduced_size = 160
out_channels = 320
kernel_size = 3
loss = "xent"
temp = 1


# List of the multivariates dataset
uea_dataset_list = uea_dataset_list()


dataset_names = []
train_score_list = []
test_score_list = []

for dataset_name in uea_dataset_list:
    
    print('Doing {}'.format(dataset_name))
    
    # First one gives *** second one gives cuda outof memory (managable), third is 'non a zip file
    if dataset_name in ['DuckDuckGeese', 'Ering']: 
        dataset_names.append(dataset_name)
        train_score_list.append(np.nan)
        test_score_list.append(np.nan)
        continue

    try:
        # Datamodule importation
        datamodule = TimeSeriesDataModule('', '', batch_size, num_workers, dataset_name, min_length=20, multivariate=True, fill_na=True)
        datamodule.setup()

        in_channels = datamodule.train_set.time_series.shape[1]

        # Model definition
        model = TimeSeriesEmbedder(
            in_channels = in_channels,
            channels = channels,
            depth = depth,
            reduced_size = reduced_size,
            out_channels = out_channels,
            kernel_size = kernel_size,
            lr=lr,
            weight_decay = weight_decay,
            betas = betas,
            train_path='',
            test_path='', 
            dataset_name=dataset_name,
            multivariate=True, 
            loss = loss,
            temp = temp)

        max_steps = 2000
        checkpoint_callback = pl.callbacks.ModelCheckpoint(
            mode="min",
            monitor="train_loss_epoch",
            dirpath="checkpoints",
            filename="causalcnn-{epoch:02d}-{train_loss_epoch:.2f}",
        )


        run = wandb.init(
                project='self-supervised-time-series',
                name='ALL EXPERIMENTS - {}'.format(dataset_name),
                tags=[dataset_name, 'Multivariate'],
                group='Multivariate SimCLR loss',
                entity='nicolas-dufour-team',
                reinit=True
                )
        wandb_logger = pl.loggers.WandbLogger()
        trainer = pl.Trainer(
            gpus=1,
            max_steps=max_steps,
            num_sanity_val_steps = -1,
            logger=wandb_logger,
            # val_check_interval=50,
            callbacks=[checkpoint_callback],
        )
        model.datamodule = datamodule
        trainer.fit(model, datamodule)
        fig = trainer.test(verbose=False)


        train_score, test_score = model.compute_scores()
        print('\n', train_score, test_score)
        dataset_name.append(dataset_name)
        wandb.log({'Train score': train_score})
        wandb.log({'Test score': test_score})
        train_score_list.append(train_score)
        test_score_list.append(test_score)
        # except:
        #     print('Dataset: {} does not have enough example per class (will fix that)'.format(os.path.basename(dir_path)))
        run.finish()
        results = pd.DataFrame(np.array([dataset_name,train_score_list, test_score_list]).transpose(), columns=['Name', 'Train Accuracy', 'Test Accuracy'])
        results.to_csv('./results_multivariate.csv')
        results.to_csv('/content/drive/MyDrive/results_multivariate.csv')

    except:
        print('Dataset: {} does not have enough example per class (will fix that)'.format(dataset_name))
        dataset_names.append(dataset_name)
        train_score_list.append(np.nan)
        test_score_list.append(np.nan)
        results = pd.DataFrame(np.array([dataset_names,train_score_list, test_score_list]).transpose(), columns=['Name', 'Train Accuracy', 'Test Accuracy'])
        results.to_csv('./results_multivariate.csv')
        results.to_csv('/content/drive/MyDrive/results_multivariate.csv')
        continue




















# Debug to look at the data

datamodule.setup(0)
train_dataloader = datamodule.train_dataloader()
test_dataloader = datamodule.test_dataloader()

labels, test_time_series = next(iter(test_dataloader))
ref_time_series, pos_time_series = next(iter(train_dataloader))

X_train, X_test , y_train , y_test = fetch_uea_dataset(dataset_name, use_cache=True, return_X_y=True)


X_train, X_test , y_train , y_test = fetch_uea_dataset('ERing', use_cache=True, return_X_y=True)


in_channels = 6
channels = 40
depth = 4
reduced_size = 160
out_channels = 320
kernel_size = 3
N_sample = 100

# The whole model
causalEncoder = CausalCNNEncoder(
    in_channels=in_channels,
    channels=channels,
    depth=depth,
    reduced_size=reduced_size,
    out_channels=out_channels,
    kernel_size=kernel_size,
).double()

# The whole model without the last global average pooling and FC between reduced_size and out_channel
causal_cnn = CausalCNN(
    in_channels=in_channels,
    channels=channels,
    depth=depth,
    out_channels=out_channels,
    kernel_size=kernel_size,
).double()


##### BUILDING EACH BLOCK OF THE MODEL

# Each of the Causal CNN Blocks (of width 4)
ConvBlock_1 = CausalConvolutionBlock(
    in_channels=in_channels, out_channels=channels, dilation=1, kernel_size=3
).double()
ConvBlock_2 = CausalConvolutionBlock(
    in_channels=channels, out_channels=channels, dilation=2, kernel_size=3
).double()
ConvBlock_3 = CausalConvolutionBlock(
    in_channels=channels, out_channels=channels, dilation=4, kernel_size=3
).double()
ConvBlock_4 = CausalConvolutionBlock(
    in_channels=channels, out_channels=reduced_size, dilation=8, kernel_size=3
).double()

# Global average pooling
reduce_size = torch.nn.AdaptiveMaxPool1d(1)

# Squeez the last (third) temporal dimension
squeeze = SqueezeChannels()

# last fully connected layer to go from reduced_size to out_channel
linear = torch.nn.Linear(reduced_size, out_channels).double()


model_from_scratch = [
    ConvBlock_1,
    ConvBlock_2,
    ConvBlock_3,
    ConvBlock_4,
    reduce_size,
    squeeze,
    linear,
]
model_name_from_scratch = [
    "ConvBlock_1",
    "ConvBlock_2",
    "ConvBlock_3",
    "ConvBlock_4",
    "Global Average Pooling",
    "squeezing",
    "final FC",
]

print("Input Shape:")
print(list(X_train.shape), "\n")
input = X_train
for block_ii in range(len(model_from_scratch)):
    print("{}:".format(model_name_from_scratch[block_ii]))
    output = model_from_scratch[block_ii](input)
    print(list(output.shape), "\n")
    input = output


# Instead of the Global Average Pooling
linear_to_squeeze = torch.nn.Linear(N_sample, 1).double()


model_from_scratch_experiment = [
    ConvBlock_1,
    ConvBlock_2,
    ConvBlock_3,
    ConvBlock_4,
    linear_to_squeeze,
    squeeze,
    linear,
]
model_name_from_scratch_experiment = [
    "ConvBlock_1",
    "ConvBlock_2",
    "ConvBlock_3",
    "ConvBlock_4",
    "linear_to_squeeze",
    "squeezing",
    "final FC",
]

print("Input Shape:")
print(list(X_train.shape), "\n")
input = X_train
for block_ii in range(len(model_from_scratch_experiment)):
    print("{}:".format(model_name_from_scratch_experiment[block_ii]))
    output = model_from_scratch_experiment[block_ii](input)
    print(list(output.shape), "\n")
    input = output


X_train, y_train, X_test, y_test = load_UCR_dataset(root_data,'DodgerLoopDay')

print("X_train: {}".format(X_train.shape))
print("y_train: {}".format(y_train.shape))
print("X_test: {}".format(X_test.shape))
print("y_test: {}".format(y_test.shape))

print('Percentage of Nan in the training set: {}'.format(100*np.isnan(X_train).sum()/(X_train.shape[0]*X_train.shape[2])))
fig,(ax1,ax2) = plt.subplots(2,1, figsize=(20,8))
fig.suptitle('Time serie before and after replaing NaN Value', weight='bold', fontsize=18)
ax1.plot(X_train[16,0,:], label='Original time serie')
nan_mask = np.isnan(X_train)
X_train[nan_mask] = np.zeros(shape=np.count_nonzero(nan_mask))
ax2.plot(X_train[16,0,:], label='Time serie transformed')
ax1.legend()
_ = ax2.legend()


train_path = os.path.join(root_data,'DodgerLoopDay','DodgerLoopDay_TRAIN.tsv')
fill_na = True
min_length = 20

data = pd.read_csv(train_path,sep='\t', header=None)
time_series = np.array(data.iloc[:,1:])

if fill_na:
    #print('Percentage of Nan in the training set: {:.2f}\nRemoving nan...'.format(100*np.isnan(self.time_series).sum()/(self.time_series.shape[0]*self.time_series.shape[1])))
    nan_mask = np.isnan(time_series)
    time_series[nan_mask] = np.zeros(shape=np.count_nonzero(nan_mask))

    
idx=0

entire_series = time_series[idx]
entire_length = entire_series.shape[0]
pos_length = np.random.randint(min_length, high = entire_length+1)
ref_length = np.random.randint(pos_length, high = entire_length+1)
ref_beg = np.random.randint(0, high = entire_length+1-ref_length)
pos_beg = np.random.randint(ref_beg, high = ref_beg+ref_length-pos_length+1)
ref_series = entire_series[ref_beg:ref_beg+ref_length]
pos_series = entire_series[pos_beg:pos_beg+pos_length]

# Visualization
plt.figure(figsize=(30,8))
plt.title('Example time serie, with reference and positive samples', weight='bold', fontsize=20)
plt.plot(entire_series, color='k', label='Original Time Series')
plt.plot(np.arange(ref_beg,ref_beg+ref_length),entire_series[ref_beg:ref_beg+ref_length], color='b', label='Ref Time Series')
plt.plot(np.arange(pos_beg,pos_beg+pos_length),entire_series[pos_beg:pos_beg+pos_length], color='r', label='Pos Time Series')
plt.xlabel('Time')
_ = plt.legend(prop={'size':20})


from pyts.datasets import load_basic_motions,uea_dataset_list, fetch_uea_dataset
X_train, X_test, y_train, y_test = fetch_uea_dataset('BasicMotions', use_cache=False, data_home='./data/Multivariate/', return_X_y=True)


# Preprocessing: normalization
X_train = (X_train - X_train.mean(axis=2)[:, :, None]) / X_train.std(axis=2)[:, :, None]
X_test = (X_test - X_test.mean(axis=2)[:, :, None]) / X_test.std(axis=2)[:, :, None]


X_train = torch.from_numpy(X_train).double()
X_test = torch.from_numpy(X_test).double()
if torch.cuda.is_available():
    X_train = X_train.cuda()
    X_test = X_test.cuda()
    
    
labels = ["Accelerometer - X", "Accelerometer - Y", "Accelerometer - Z", "Gyroscope - Yaw", "Gyroscope - Pitch", "Gyroscope - Roll"]
if True:
    index = 0
    fig = plt.figure(figsize=(30,15))
    axes = []
    for i in range(6):
        axes.append(fig.add_subplot(6,2,2*i+1))
        axes[i].set_title('{}'.format(str(y_train[index])[2:-1]), weight = 'bold', fontsize=18)
        axes[i].plot(X_train[index,i,:], label = labels[i])
        axes[i].legend(loc=1)
    index = -1
    for i in range(6):
        axes.append(fig.add_subplot(6,2,2*(i+1)))
        axes[6+i].set_title('{}'.format(str(y_train[index])[2:-1]), weight = 'bold', fontsize=18)
        axes[6+i].plot(X_train[index,i,:], label = labels[i])
        axes[6+i].legend(loc=1)
plt.tight_layout()



time_series = X_train


idx=0
min_length = 20
entire_series = time_series[idx]
entire_length = entire_series.shape[1]
pos_length = np.random.randint(min_length, high = entire_length+1)
ref_length = np.random.randint(pos_length, high = entire_length+1)
ref_beg = np.random.randint(0, high = entire_length+1-ref_length)
pos_beg = np.random.randint(ref_beg, high = ref_beg+ref_length-pos_length+1)

ref_series = entire_series[:,ref_beg:ref_beg+ref_length]
pos_series = entire_series[:,pos_beg:pos_beg+pos_length]



labels = ["Accelerometer - X", "Accelerometer - Y", "Accelerometer - Z", "Gyroscope - Yaw", "Gyroscope - Pitch", "Gyroscope - Roll"]
if True:
    index = 0
    min_length = 20
    entire_series = time_series[index]
    entire_length = entire_series.shape[1]
    pos_length = np.random.randint(min_length, high = entire_length+1)
    ref_length = np.random.randint(pos_length, high = entire_length+1)
    ref_beg = np.random.randint(0, high = entire_length+1-ref_length)
    pos_beg = np.random.randint(ref_beg, high = ref_beg+ref_length-pos_length+1)

    ref_series = entire_series[:,ref_beg:ref_beg+ref_length]
    pos_series = entire_series[:,pos_beg:pos_beg+pos_length]

    fig = plt.figure(figsize=(30,15))
    axes = []
    for i in range(6):
        axes.append(fig.add_subplot(6,2,2*i+1))
        axes[i].set_title('{}'.format(str(y_train[index])[2:-1]), weight = 'bold', fontsize=18)
        #axes[i].plot(X_train[index,i,:], label = labels[i])
        axes[i].plot(entire_series[i,:], color='k', label='Entire Serie')
        axes[i].plot(np.arange(ref_beg,ref_beg+ref_length),entire_series[i,ref_beg:ref_beg+ref_length], color='b', label='Ref Time Series')
        axes[i].plot(np.arange(pos_beg,pos_beg+pos_length),entire_series[i,pos_beg:pos_beg+pos_length], color='r', label='Pos Time Series')
        axes[i].legend(loc=1)
        
    index = -1
    min_length = 20
    entire_series = time_series[index]
    entire_length = entire_series.shape[1]
    pos_length = np.random.randint(min_length, high = entire_length+1)
    ref_length = np.random.randint(pos_length, high = entire_length+1)
    ref_beg = np.random.randint(0, high = entire_length+1-ref_length)
    pos_beg = np.random.randint(ref_beg, high = ref_beg+ref_length-pos_length+1)
    ref_series = entire_series[:,ref_beg:ref_beg+ref_length]
    pos_series = entire_series[:,pos_beg:pos_beg+pos_length]

    for i in range(6):
        axes.append(fig.add_subplot(6,2,2*(i+1)))
        axes[6+i].set_title('{}'.format(str(y_train[index])[2:-1]), weight = 'bold', fontsize=18)
        #axes[6+i].plot(X_train[index,i,:], label = labels[i])
        axes[6+i].plot(entire_series[i,:], color='k', label='Entire Serie')
        axes[6+i].plot(np.arange(ref_beg,ref_beg+ref_length),entire_series[i,ref_beg:ref_beg+ref_length], color='b', label='Ref Time Series')
        axes[6+i].plot(np.arange(pos_beg,pos_beg+pos_length),entire_series[i,pos_beg:pos_beg+pos_length], color='r', label='Pos Time Series')
        axes[6+i].legend(loc=1)
plt.tight_layout()








# Save to git
get_ipython().getoutput("git config --global user.email "nicolas.dufourn@gmail.com"")
get_ipython().getoutput("git config --global user.name "Nicolas DUFOUR"")
get_ipython().getoutput("git add --all")
get_ipython().getoutput("git commit -m "Fixed val logging"")
get_ipython().getoutput("git push")


get_ipython().run_line_magic("ls", "")



